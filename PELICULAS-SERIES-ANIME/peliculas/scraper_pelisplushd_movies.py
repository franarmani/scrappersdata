"""
Scraper de pel√≠culas desde verpeliculasultra.com
Extrae: t√≠tulo, a√±o, servidores disponibles
Busca tmdb_id en TMDB
Guarda estructura simplificada: {tmdb_id, title, year, servers}
Con sincronizaci√≥n autom√°tica a GitHub y Supabase
"""

import json
import time
import logging
from urllib.parse import urljoin
from typing import List, Dict, Optional
import requests
from bs4 import BeautifulSoup
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
import os
import sys
import subprocess
from datetime import datetime

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Configuraci√≥n TMDB
TMDB_API_KEY = "201d333198374a91c81dba3c443b1a8e"
TMDB_BASE_URL = "https://api.themoviedb.org/3"

class VerpeliculasUltraaScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.movies = []
        self.processed_tmdb_ids = set()
        
    def buscar_tmdb(self, title: str, year: str) -> Optional[Dict]:
        """Busca pel√≠cula en TMDB y retorna su informaci√≥n"""
        try:
            # Limpiar t√≠tulo para b√∫squeda
            search_title = title.split('(')[0].strip() if '(' in title else title
            
            params = {
                'api_key': TMDB_API_KEY,
                'query': search_title,
                'language': 'es-ES',
                'page': 1
            }
            
            response = self.session.get(f"{TMDB_BASE_URL}/search/movie", params=params, timeout=10)
            response.raise_for_status()
            
            results = response.json().get('results', [])
            
            if not results:
                logger.warning(f"No se encontr√≥ en TMDB: {search_title}")
                return None
            
            # Buscar coincidencia por a√±o si est√° disponible
            best_match = None
            for movie in results:
                release_date = movie.get('release_date', '')
                movie_year = release_date.split('-')[0] if release_date else ''
                
                # Coincidir por a√±o si est√° disponible
                if year and movie_year == year:
                    best_match = movie
                    break
            
            # Si no hay coincidencia de a√±o, usar el primer resultado
            best_match = best_match or results[0]
            
            return {
                'tmdb_id': best_match.get('id'),
                'title': best_match.get('title'),
                'year': best_match.get('release_date', '').split('-')[0] if best_match.get('release_date') else year
            }
            
        except Exception as e:
            logger.error(f"Error buscando en TMDB '{title}': {e}")
            return None
    
    def extraer_servidores(self, url: str) -> List[Dict]:
        """Extrae servidores y sus URLs de la p√°gina de pel√≠cula"""
        servidores = []
        servidores_vistos = set()
        
        try:
            response = self.session.get(url, timeout=15)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Buscar todos los tabs con sus idiomas
            tabs_sidebar = soup.find('div', class_='tabs-sidebar')
            if not tabs_sidebar:
                logger.warning(f"No se encontr√≥ tabs-sidebar en {url}")
                return servidores
            
            # Obtener los tabs ul li para extraer idiomas
            tabs_ul = tabs_sidebar.find('ul', class_='tabs-sidebar-ul')
            idiomas_info = {}
            
            if tabs_ul:
                for idx, li in enumerate(tabs_ul.find_all('li')):
                    link = li.find('a')
                    if link:
                        href = link.get('href')
                        span = link.find('span')
                        idioma = span.get_text(strip=True) if span else f"Tab {idx}"
                        if href and href.startswith('#'):
                            tab_id = href[1:]  # Quitar el #
                            idiomas_info[tab_id] = idioma
            
            # Buscar los bloques de tabs
            tabs_blocks = tabs_sidebar.find_all('div', class_='tabs-sidebar-block')
            
            for block in tabs_blocks:
                block_id = block.get('id', '')
                idioma = idiomas_info.get(block_id, 'Desconocido')
                
                # Buscar el mejs-container
                video_div = block.find('div', class_='mejs-container')
                if video_div:
                    data_src = video_div.get('data-src')
                    if data_src:
                        # Extraer informaci√≥n del servidor
                        from urllib.parse import urlparse
                        parsed = urlparse(data_src)
                        domain = parsed.netloc.replace('www.', '')
                        
                        # Crear clave √∫nica para evitar duplicados exactos
                        key = f"{domain}|{data_src}"
                        
                        if key not in servidores_vistos:
                            servidor_info = {
                                'url': data_src,
                                'server': domain,
                                'language': idioma
                            }
                            servidores.append(servidor_info)
                            servidores_vistos.add(key)
            
            logger.info(f"Servidores encontrados: {len(servidores)} ({', '.join([s['server'] for s in servidores])})")
            
        except Exception as e:
            logger.error(f"Error extrayendo servidores de {url}: {e}")
            import traceback
            logger.debug(traceback.format_exc())
        
        return servidores
    
    def extraer_pel√≠cula_info(self, url: str) -> Optional[Dict]:
        """Extrae informaci√≥n de una pel√≠cula desde su p√°gina individual"""
        try:
            response = self.session.get(url, timeout=15)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Buscar t√≠tulo en #bar-film
            bar_film = soup.find('section', id='bar-film')
            if not bar_film:
                logger.warning(f"No se encontr√≥ #bar-film en {url}")
                return None
            
            # Extraer t√≠tulo
            titulo_elem = bar_film.find('span', class_='f-info-text')
            titulo = titulo_elem.get_text(strip=True) if titulo_elem else None
            
            if not titulo:
                logger.warning(f"No se encontr√≥ t√≠tulo en {url}")
                return None
            
            # Extraer a√±o
            year_elem = None
            for li in bar_film.find_all('li'):
                span_title = li.find('span', class_='f-info-title')
                if span_title and 'A√±o' in span_title.get_text():
                    year_elem = li.find('span', class_='f-info-text')
                    break
            
            year = year_elem.get_text(strip=True) if year_elem else ''
            
            logger.info(f"Pel√≠cula encontrada: {titulo} ({year})")
            
            return {
                'title': titulo,
                'year': year,
                'url': url
            }
            
        except Exception as e:
            logger.error(f"Error extrayendo informaci√≥n de {url}: {e}")
            return None
    
    def extraer_grid_pel√≠culas(self, page_url: str) -> List[Dict]:
        """Extrae URLs de pel√≠culas desde la p√°gina de grid"""
        pel√≠culas = []
        try:
            response = self.session.get(page_url, timeout=15)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Buscar todos los divs con clase shortf
            shortf_divs = soup.find_all('div', class_='shortf')
            
            logger.info(f"Se encontraron {len(shortf_divs)} pel√≠culas en la p√°gina")
            
            for shortf in shortf_divs:
                # Buscar el link de la pel√≠cula
                link_elem = shortf.find('a', href=True)
                if link_elem and link_elem.get('href'):
                    url = link_elem['href']
                    pel√≠culas.append({'url': url})
            
            logger.info(f"Extra√≠das {len(pel√≠culas)} URLs de pel√≠culas")
            
        except Exception as e:
            logger.error(f"Error extrayendo grid de {page_url}: {e}")
        
        return pel√≠culas
    
    def procesar_pel√≠culas(self, page_url: str = "https://verpeliculasultra.com/lastnews/", max_pages: int = 1):
        """Procesa pel√≠culas desde las p√°ginas de grid"""
        for page_num in range(1, max_pages + 1):
            if page_num == 1:
                url = page_url
            else:
                url = f"{page_url}page/{page_num}/"
            
            logger.info(f"Procesando p√°gina {page_num}: {url}")
            
            # Extraer URLs del grid
            pel√≠culas_urls = self.extraer_grid_pel√≠culas(url)
            
            for idx, pel√≠cula_info in enumerate(pel√≠culas_urls):
                try:
                    logger.info(f"Procesando pel√≠cula {idx + 1}/{len(pel√≠culas_urls)}")
                    
                    # Extraer informaci√≥n de la pel√≠cula
                    info = self.extraer_pel√≠cula_info(pel√≠cula_info['url'])
                    if not info:
                        continue
                    
                    # Buscar en TMDB
                    tmdb_info = self.buscar_tmdb(info['title'], info['year'])
                    if not tmdb_info or not tmdb_info.get('tmdb_id'):
                        logger.warning(f"No se encontr√≥ tmdb_id para {info['title']}")
                        continue
                    
                    # Evitar duplicados
                    if tmdb_info['tmdb_id'] in self.processed_tmdb_ids:
                        logger.info(f"Pel√≠cula ya procesada: {tmdb_info['title']}")
                        continue
                    
                    # Extraer servidores
                    servidores = self.extraer_servidores(pel√≠cula_info['url'])
                    
                    # Crear estructura simplificada
                    pel√≠cula = {
                        'tmdb_id': tmdb_info['tmdb_id'],
                        'title': tmdb_info['title'],
                        'year': tmdb_info['year'],
                        'servers': servidores
                    }
                    
                    self.movies.append(pel√≠cula)
                    self.processed_tmdb_ids.add(tmdb_info['tmdb_id'])
                    
                    logger.info(f"Pel√≠cula a√±adida: {pel√≠cula['title']} ({pel√≠cula['year']}) - Servidores: {len(servidores)}")
                    
                    # Peque√±a pausa para no saturar servidores
                    time.sleep(1)
                    
                except Exception as e:
                    logger.error(f"Error procesando pel√≠cula {pel√≠cula_info.get('url')}: {e}")
                    continue
            
            # Pausa entre p√°ginas
            time.sleep(2)
    
    def guardar_pel√≠culas(self, output_file: str = '../peliculas.json'):
        """Guarda pel√≠culas en JSON con estructura simplificada"""
        try:
            # Obtener ruta absoluta
            script_dir = os.path.dirname(os.path.abspath(__file__))
            full_path = os.path.join(script_dir, output_file)
            
            # Leer pel√≠culas existentes
            pel√≠culas_existentes = {}
            if os.path.exists(full_path):
                with open(full_path, 'r', encoding='utf-8') as f:
                    try:
                        data = json.load(f)
                        # Convertir a diccionario con tmdb_id como clave
                        pel√≠culas_existentes = {p['tmdb_id']: p for p in data}
                    except:
                        pass
            
            # Actualizar con nuevas pel√≠culas
            for pel√≠cula in self.movies:
                pel√≠culas_existentes[pel√≠cula['tmdb_id']] = pel√≠cula
            
            # Convertir a lista y guardar
            pel√≠culas_finales = list(pel√≠culas_existentes.values())
            
            with open(full_path, 'w', encoding='utf-8') as f:
                json.dump(pel√≠culas_finales, f, indent=2, ensure_ascii=False)
            
            logger.info(f"‚úÖ Guardadas {len(pel√≠culas_finales)} pel√≠culas en {full_path}")
            
        except Exception as e:
            logger.error(f"Error guardando pel√≠culas: {e}")
    
    def run(self, max_pages: int = 1):
        """Ejecuta el scraper"""
        logger.info("Iniciando scraper de verpeliculasultra.com...")
        
        try:
            self.procesar_pel√≠culas(max_pages=max_pages)
            self.guardar_pel√≠culas()
            logger.info(f"‚úÖ Scraping completado. Total: {len(self.movies)} pel√≠culas")
            
            # Sincronizar autom√°ticamente con GitHub y Supabase
            self.sync_to_github()
            self.sync_to_supabase()
            
        except KeyboardInterrupt:
            logger.info("Scraper interrumpido por el usuario")
        except Exception as e:
            logger.error(f"Error en el scraper: {e}")
    
    def sync_to_github(self):
        """Sincroniza autom√°ticamente con GitHub"""
        try:
            logger.info("\nüöÄ SINCRONIZANDO CON GITHUB")
            logger.info("=" * 40)
            
            # Obtener ruta del proyecto ra√≠z
            script_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(script_dir)))
            
            # Cambiar a directorio del proyecto
            os.chdir(project_root)
            
            # Ejecutar comandos git
            logger.info("üìù Preparando cambios...")
            subprocess.run(['git', 'add', 'PELICULAS-SERIES-ANIME/peliculas.json'], 
                          check=True, capture_output=True)
            
            # Crear commit con timestamp
            commit_msg = f"Update: peliculas.json - {len(self.movies)} movies from verpeliculasultra.com [{datetime.now().strftime('%Y-%m-%d %H:%M')}]"
            result = subprocess.run(['git', 'commit', '-m', commit_msg], 
                                   capture_output=True, text=True)
            
            if result.returncode == 0:
                logger.info(f"‚úÖ Commit creado: {commit_msg[:50]}...")
            else:
                logger.warning("‚ö†Ô∏è No hay cambios para commit o ya existe")
                return
            
            # Pull remoto para traer cambios
            logger.info("üì• Trayendo cambios del remoto...")
            subprocess.run(['git', 'pull', 'origin', 'master', '--rebase'], 
                          check=False, capture_output=True)
            
            # Push a remoto
            logger.info("üì§ Subiendo a GitHub...")
            result = subprocess.run(['git', 'push', 'origin', 'master'], 
                                   capture_output=True, text=True)
            
            if result.returncode == 0:
                logger.info("üéâ ¬°Push a GitHub exitoso!")
            else:
                logger.error(f"‚ùå Error en push: {result.stderr}")
        
        except subprocess.CalledProcessError as e:
            logger.error(f"‚ö†Ô∏è Error ejecutando git: {e}")
            logger.info("üí° Git command failed, but JSON saved successfully")
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Error sincronizando con GitHub: {e}")
            logger.info("üí° JSON file saved successfully")
    
    def sync_to_supabase(self):
        """Sincroniza autom√°ticamente con Supabase"""
        try:
            logger.info(f"\nüöÄ SINCRONIZANDO CON SUPABASE")
            logger.info("=" * 40)
            
            # Importar m√≥dulo de sincronizaci√≥n
            from sync_movies_supabase import MoviesSuabaseSync
            
            # Crear instancia del sincronizador
            supabase_sync = MoviesSuabaseSync()
            
            # Inicializar conexi√≥n a Supabase
            if not supabase_sync.initialize_supabase():
                logger.error("‚ùå No se pudo inicializar la conexi√≥n a Supabase")
                return
            
            # Leer pel√≠culas del archivo
            script_dir = os.path.dirname(os.path.abspath(__file__))
            json_path = os.path.join(script_dir, '../peliculas.json')
            
            with open(json_path, 'r', encoding='utf-8') as f:
                movies = json.load(f)
            
            # Sincronizar pel√≠culas
            success = supabase_sync.sync_movies_to_supabase(movies)
            
            if success:
                logger.info("üéâ ¬°Pel√≠culas sincronizadas con Supabase exitosamente!")
            else:
                logger.warning("‚ö†Ô∏è Error durante la sincronizaci√≥n con Supabase")
                
        except ImportError:
            logger.warning("‚ö†Ô∏è M√≥dulo de sincronizaci√≥n Supabase no disponible")
            logger.info("üí° Instala las dependencias: pip install supabase python-dotenv")
        except FileNotFoundError:
            logger.error("‚ö†Ô∏è Archivo peliculas.json no encontrado")
        except Exception as e:
            error_msg = str(e)
            if "getaddrinfo failed" in error_msg:
                logger.warning("‚ö†Ô∏è Sin conexi√≥n a internet, sincronizaci√≥n omitida")
                logger.info("üí° El archivo JSON se guard√≥ correctamente")
            else:
                logger.error(f"‚ö†Ô∏è Error sincronizando con Supabase: {e}")
                logger.info("üí° El archivo JSON se guard√≥ correctamente")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='Scraper de pel√≠culas desde verpeliculasultra.com')
    parser.add_argument('--max-pages', type=int, default=1, help='N√∫mero m√°ximo de p√°ginas a scrapear')
    parser.add_argument('--output', type=str, default='../peliculas.json', help='Archivo de salida JSON')
    
    args = parser.parse_args()
    
    scraper = VerpeliculasUltraaScraper()
    scraper.run(max_pages=args.max_pages)


if __name__ == '__main__':
    main()
